# -*- coding: utf-8 -*-
"""stemdtw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x6tKTyh9Y6mXWwQhKRJAYrETdFQl8F5F

# Translation
"""

import nltk
#nltk.download('punkt')
from deep_translator import GoogleTranslator
to_translate = 'Bonjour.'
translated = GoogleTranslator(source='auto', target='en').translate(to_translate)
print(translated)

"""# Plagiarism algortihm"""

from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter
import numpy as np
from fastdtw import fastdtw
from scipy.spatial.distance import euclidean
import nltk

lemmatizer = WordNetLemmatizer
porter = PorterStemmer()
lancaster = LancasterStemmer()
snowballEN = SnowballStemmer("english")


def stemSentence(sentence, language='english'):
    token_words=word_tokenize(sentence)
    stem_sentence=[]
    for word in token_words:
        stem_sentence.append(SnowballStemmer(language).stem(word)) #stem_sentence.append(snowball.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)

file1 = open('english.txt')
file1 = (file1.readline()).lower()

file2 = open('text2.txt')
file2 = (file2.readline()).lower()


threshold = input('What is your keyword threshold: ')
tokenized1 = stemSentence(file1, 'english')
tokenized2 = stemSentence(file2, 'english')

again1 = word_tokenize(tokenized1)
again2 = word_tokenize(tokenized2)
counts = Counter(again1)

print(counts)

keywords = []
for key in counts:
  if counts[key] >= int(threshold):
    keywords.append(key)
    #print(key, counts[key])

keywords = [word for word in keywords if word not in ('and', '.', 'the')]
print(keywords)

binary1 = []
binary2 = []
for word in again1:
  if word in keywords:
    binary1.append(1)
  else:
    binary1.append(0)
print(binary1)

for word in again2:
  if word in keywords:
    binary2.append(1)
  else:
    binary2.append(0)
print(binary2)

#the window size
window_size = 3
step_size = 2
start = 0

#assert len(binary1) % step_size == 0
sequence1 = []
sequence2 = []

for i in range(0,100):
  windows = binary1[start:start+window_size]
  sequence1.append(sum(windows))
  start += step_size
  if start+step_size >= len(binary1):
    break
#print(sequence1)

start = 0
for i in range(0,100):
  windows = binary2[start:start+window_size]
  sequence2.append(sum(windows))
  start += step_size
  if start+step_size >= len(binary2):
    break
#print(sequence2)

x = np.array(sequence1)
y = np.array(sequence2)

distance, path = fastdtw(x, y, dist=euclidean)

print(distance)
#print(path)

"""# DTW from scratch"""

from scipy import stats
import numpy as np

def dtw(s, t):
    n, m = len(s), len(t)
    dtw_matrix = np.zeros((n + 1, m + 1))
    
    for i in range(n + 1):
        for j in range(m + 1):
            dtw_matrix[i, j] = np.inf
    dtw_matrix[0, 0] = 0

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            cost = abs(s[i - 1] - t[j - 1])
            # take last min from a square box
            last_min = np.min([dtw_matrix[i - 1, j], dtw_matrix[i, j - 1], dtw_matrix[i - 1, j - 1]])
            dtw_matrix[i, j] = cost + last_min
            print('cost: ', cost, 'lastmin: ', last_min, 'dtw: ', str(cost+last_min))
    return dtw_matrix

a = [1,2,3]
b = [2,2,2,3,4]
dtw(a,b)

"""# Simple example, fastdtw"""

from fastdtw import fastdtw
from scipy.spatial.distance import euclidean

x = np.array([1, 2, 3])
y = np.array([2, 2, 2, 3, 4])

distance, path = fastdtw(x, y, dist=euclidean)

print(distance)
print(path)

# 5.0
# [(0, 0), (1, 1), (1, 2), (1, 3), (1, 4), (2, 5), (3, 6), (4, 7)]



"""# DTW for KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn import preprocessing
import numpy as np
from fastdtw import fastdtw
from scipy.spatial.distance import euclidean
from sklearn.metrics import classification_report, confusion_matrix

# First feature
weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',
'Rainy','Sunny','Overcast','Overcast','Rainy']

# Second Feature
temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']

# Label or target varible
play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']

#creating labelEncoder
le = preprocessing.LabelEncoder()
# Converting string labels into numbers.
weather_encoded=le.fit_transform(weather)

temp_encoded=le.fit_transform(temp)
label=le.fit_transform(play)
features=list(zip(weather_encoded,temp_encoded))


def levels2binary(input):
  
  # removing duplicates by converting to dictionary and back
  input = list(dict.fromkeys(input))
  
  i=0
  while i<len(input):
    input[i] = i
    i+=1
  return input

levels2binary(weather)

def dtw_distance(self, ts_a, ts_b, d = lambda x,y: abs(x-y)):
# accepts 2D numpy arrays
# DistanceMetric default = abs(x-y)
        # cost matrix
        ts_a, ts_b = np.array(ts_a), np.array(ts_b)
        M, N = len(ts_a), len(ts_b)
        cost = float("inf") * np.ones((M, N))

        # Initialize the first row and column
        cost[0, 0] = d(ts_a[0], ts_b[0])
        for i in xrange(1, M):
            cost[i, 0] = cost[i-1, 0] + d(ts_a[i], ts_b[0])

        for j in xrange(1, N):
            cost[0, j] = cost[0, j-1] + d(ts_a[0], ts_b[j])

        # Populate rest of cost matrix within window
        for i in xrange(1, M):
            for j in xrange(max(1, i - max_warping_window),
                            min(N, i + max_warping_window)):
                choices = cost[i - 1, j - 1], cost[i, j-1], cost[i-1, j]
                cost[i, j] = min(choices) + d(ts_a[i], ts_b[j])

        # Return DTW distance given window 
        return cost[-1, -1]

def dist_matrix(x, y): 
        else:
            x_s = np.shape(x)
            y_s = np.shape(y)
            dm = np.zeros((x_s[0], y_s[0])) 
            dm_size = x_s[0]*y_s[0]
            
            p = ProgressBar(dm_size)
        
            for i in xrange(0, x_s[0]):
                for j in xrange(0, y_s[0]):
                    dm[i, j] = self._dtw_distance(x[i, ::step],
                                                  y[j, ::step])
                    # Update progress bar
                    dm_count += 1
                    p.animate(dm_count)
        
            return dm
def predict(x):
        
        dist_matrix(x, x)

        # Identify the k nearest neighbors
        knn_idx = dm.argsort()[:, :n_neighbors]

        # Identify k nearest labels
        knn_labels = l[knn_idx]
        
        # Model Label
        mode_data = mode(knn_labels, axis=1)
        mode_label = mode_data[0]
        mode_proba = mode_data[1]/n_neighbors

        return mode_label.ravel(), mode_proba.ravel()

x = np.array(temp_encoded)
y = np.array(label)

distance, path = fastdtw(x, y, dist=euclidean)
print(distance)

def dtw(x, y):
  result = fastdtw(x, y, dist=euclidean)[0]
  return result

model = KNeighborsClassifier(n_neighbors=3, metric = dtw)

# Train the model using the training sets
model.fit(temp_encoded.reshape(-1,1),label)

#Predict output
predicted= model.predict([[0]])
print(predicted)

conf_mat = confusion_matrix(label, y_test[::10])

fig = plt.figure(figsize=(6,6))
width = np.shape(conf_mat)[1]
height = np.shape(conf_mat)[0]

res = plt.imshow(np.array(conf_mat), cmap=plt.cm.summer, interpolation='nearest')
for i, row in enumerate(conf_mat):
    for j, c in enumerate(row):
        if c>0:
            plt.text(j-.2, i+.1, c, fontsize=16)
            
cb = fig.colorbar(res)
plt.title('Confusion Matrix')
plt.xticks(range(6), [l for l in labels.values()], rotation=90)
plt.yticks(range(6), [l for l in labels.values()])

"""# DTW for speech recognition"""

