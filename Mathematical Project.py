# -*- coding: utf-8 -*-
"""attmept2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m7A0o-fs5PdRwxxPIRzjWkJxNkJH0FLe

# **Preprocessing**
"""

from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from collections import Counter

porter = PorterStemmer()
lancaster = LancasterStemmer()

import nltk

def stemSentence(sentence):
    token_words=word_tokenize(sentence)
    stem_sentence=[]
    for word in token_words:
        stem_sentence.append(lancaster.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)

file = open('english.txt')
file = (file.readline()).lower()

threshold = input('What is your keyword threshold: ')
tokenized = stemSentence(file)
again = word_tokenize(tokenized)
counts = Counter(again)
print(counts)
keywords = []
for key in counts:
  if counts[key] >= int(threshold):
    keywords.append(key)
    print(key, counts[key])

keywords = [word for word in keywords if word not in ('and', '.', 'the')]
print(keywords)

"""# **Converting to binary**"""

binary = []
for word in again:
  if word in keywords:
    binary.append(1)
  else:
    binary.append(0)
print(binary)


#the window size
window_size = 3
step_size = 2
start = 0

print(len(binary) % step_size == 0)
sequence = []

for i in range(0,100):
  windows = binary[start:start+window_size]
  sequence.append(sum(windows))
  start += step_size
  if start+step_size >= len(binary):
    break
print(sequence)

"""# **DTW algortihm**"""

import numpy as np
from fastdtw import fastdtw
from scipy.spatial.distance import euclidean

x = np.array(sequence)
y = np.array([2, 2, 2, 3, 4])

distance, path = fastdtw(x, y, dist=euclidean)

print(distance)
print(path)

